<?xml version="1.0" encoding="UTF-8"?><xml><records><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Pedregosa, Fabian</author><author>Varoquaux, Gaeel</author><author>Gramfort, Alexandre</author><author>Michel, Vincent</author><author>Thirion, Bertrand</author><author>Grisel, Olivier</author><author>Blondel, Mathieu</author><author>Prettenhofer, Peter</author><author>Weiss, Ron</author><author>Dubourg, Vincent</author><author>Vanderplas, Jake</author><author>Passos, Alexandre</author><author>Cournapeau, David</author><author>Brucher, Matthieu</author><author>Perrot, Matthieu</author><author>Duchesnay, Edouard</author></authors></contributors><titles><title>Scikit-learn: Machine Learning in Python</title><secondary-title>JOURNAL OF MACHINE LEARNING RESEARCH</secondary-title></titles><periodical><full-title>JOURNAL OF MACHINE LEARNING RESEARCH</full-title></periodical><pages>2825-2830</pages><volume>12</volume><keywords><keyword>Python; supervised learning; unsupervised learning; model selection</keyword></keywords><dates><year>2011</year></dates><urls/><abstract>Scikit-learn is a Python module integrating a wide range of
state-of-the-art machine learning algorithms for medium-scale supervised
and unsupervised problems. This package focuses on bringing machine
learning to non-specialists using a general-purpose high-level language.
Emphasis is put on ease of use, performance, documentation, and API
consistency. It has minimal dependencies and is distributed under the
simplified BSD license, encouraging its use in both academic and
commercial settings. Source code, binaries, and documentation can be
downloaded from http://scikit-learn.sourceforge.net.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>CORTES, C</author><author>VAPNIK, V</author></authors></contributors><titles><title>SUPPORT-VECTOR NETWORKS</title><secondary-title>MACHINE LEARNING</secondary-title></titles><periodical><full-title>MACHINE LEARNING</full-title></periodical><pages>273-297</pages><volume>20</volume><issue>3</issue><keywords><keyword>PATTERN RECOGNITION; EFFICIENT LEARNING ALGORITHMS; NEURAL NETWORKS; RADIAL BASIS FUNCTION CLASSIFIERS; POLYNOMIAL CLASSIFIERS</keyword></keywords><dates><year>1995</year></dates><electronic-resource-num>10.1007/BF00994018</electronic-resource-num><urls/><abstract>The support-vector network is a new learning machine for two-group
classification problems. The machine conceptually implements the
following idea: input vectors are non-linearly mapped to a very
high-dimension feature space. In this feature space a linear decision
surface is constructed. Special properties of the decision surface
ensures high generalization ability of the learning machine. The idea
behind the support-vector network was previously implemented for the
restricted case where the training data can be separated without errors.
We here extend this result to non-separable training data.
High generalization ability of support-vector networks utilizing
polynomial input transformations is demonstrated. We also compare the
performance of the support-vector network to various classical learning
algorithms that all took part in a benchmark study of Optical Character
Recognition.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Lecun, Y</author><author>Bottou, L</author><author>Bengio, Y</author><author>Haffner, P</author></authors></contributors><titles><title>Gradient-based learning applied to document recognition</title><secondary-title>PROCEEDINGS OF THE IEEE</secondary-title></titles><periodical><full-title>PROCEEDINGS OF THE IEEE</full-title></periodical><pages>2278-2324</pages><volume>86</volume><issue>11</issue><keywords><keyword>convolutional neural networks; document recognition; finite state transducers; gradient-based learning; graph transformer networks; machine learning; neural networks; optical character recognition (OCR)</keyword></keywords><dates><year>1998</year></dates><electronic-resource-num>10.1109/5.726791</electronic-resource-num><urls/><abstract>Multilayer neural networks trained with the back-propagation algorithm
constitute the best example of a successful gradient-based learning
technique. Given an appropriate network architecture, gradient-based
learning algorithms can be used to synthesize a complex decision surface
that can classify high-dimensional patterns, arch as handwritten
characters, with minimal preprocessing. This paper reviews various
methods applied to handwritten character recognition and compares them
on a standard handwritten digit recognition task. Convolutional neural
networks, which are specifically designed to deal with the variability
of two dimensional (2-D) shapes, are shown to outperform all other
techniques.
Real-life document recognition systems are composed of multiple modules
including field extraction, segmentation recognition and language
modeling. A new learning paradigm, called graph transformer networks
(GTN's), allows such multimodule systems to be trained globally using
gradient-based methods so; as to minimize an overall performance
measure.
Two systems for online handwriting recognition are described.
Experiments demonstrate the advantage of global training, and the
flexibility of graph transformer networks.
A graph transformer network for reading a bank check is also described
It uses convolutional neural network character recognizers combined with
global training techniques to provide record accuracy on business and
personal checks. It is deployed commercially and reads several million
checks per day.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>van der Maaten, Laurens</author><author>Hinton, Geoffrey</author></authors></contributors><titles><title>Visualizing Data using t-SNE</title><secondary-title>JOURNAL OF MACHINE LEARNING RESEARCH</secondary-title></titles><periodical><full-title>JOURNAL OF MACHINE LEARNING RESEARCH</full-title></periodical><pages>2579-2605</pages><volume>9</volume><keywords><keyword>visualization; dimensionality reduction; manifold learning; embedding algorithms; multidimensional scaling</keyword></keywords><dates><year>2008</year></dates><urls/><abstract>We present a new technique called ``t-SNE{''} that visualizes
high-dimensional data by giving each datapoint a location in a two or
three-dimensional map. The technique is a variation of Stochastic
Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to
optimize, and produces significantly better visualizations by reducing
the tendency to crowd points together in the center of the map. t-SNE is
better than existing techniques at creating a single map that reveals
structure at many different scales. This is particularly important for
high-dimensional data that lie on several different, but related,
low-dimensional manifolds, such as images of objects from multiple
classes seen from multiple viewpoints. For visualizing the structure of
very large data sets, we show how t-SNE can use random walks on
neighborhood graphs to allow the implicit structure of all of the data
to influence the way in which a subset of the data is displayed. We
illustrate the performance of t-SNE on a wide variety of data sets and
compare it with many other non-parametric visualization techniques,
including Sammon mapping, Isomap, and Locally Linear Embedding. The
visualizations produced by t-SNE are significantly better than those
produced by the other techniques on almost all of the data sets.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Breiman, L</author></authors></contributors><titles><title>Random forests</title><secondary-title>MACHINE LEARNING</secondary-title></titles><periodical><full-title>MACHINE LEARNING</full-title></periodical><pages>5-32</pages><volume>45</volume><issue>1</issue><keywords><keyword>classification; regression; ensemble</keyword></keywords><dates><year>2001</year></dates><electronic-resource-num>10.1023/A:1010933404324</electronic-resource-num><urls/><abstract>Random forests are a combination of tree predictors such that each tree
depends on the values of a random vector sampled independently and with
the same distribution for all trees in the forest. The generalization
error for forests converges a.s. to a limit as the number of trees in
the forest becomes large. The generalization error of a forest of tree
classifiers depends on the strength of the individual trees in the
forest and the correlation between them. Using a random selection of
features to split each node yields error rates that compare favorably to
Adaboost (Y. Freund \&amp; R. Schapire, Machine Learning: Proceedings of the
Thirteenth International conference, {*}{*}{*}, 148-156), but are more
robust with respect to noise. Internal estimates monitor error,
strength, and correlation and these are used to show the response to
increasing the number of features used in the splitting. Internal
estimates are also used to measure variable importance. These ideas are
also applicable to regression.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Chang, Chih-Chung</author><author>Lin, Chih-Jen</author></authors></contributors><titles><title>LIBSVM: A Library for Support Vector Machines</title><secondary-title>ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY</secondary-title></titles><periodical><full-title>ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY</full-title></periodical><volume>2</volume><issue>3, SI</issue><keywords><keyword>Algorithms; Performance; Experimentation; Classification LIBSVM optimization regression support vector machines SVM</keyword></keywords><dates><year>2011</year></dates><electronic-resource-num>10.1145/1961189.1961199</electronic-resource-num><urls/><abstract>LIBSVM is a library for Support Vector Machines (SVMs). We have been
actively developing this package since the year 2000. The goal is to
help users to easily apply SVM to their applications. LIBSVM has gained
wide popularity in machine learning and many other areas. In this
article, we present all implementation details of LIBSVM. Issues such as
solving SVM optimization problems theoretical convergence multiclass
classification probability estimates and parameter selection are
discussed in detail.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Srivastava, Nitish</author><author>Hinton, Geoffrey</author><author>Krizhevsky, Alex</author><author>Sutskever, Ilya</author><author>Salakhutdinov, Ruslan</author></authors></contributors><titles><title>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title><secondary-title>JOURNAL OF MACHINE LEARNING RESEARCH</secondary-title></titles><periodical><full-title>JOURNAL OF MACHINE LEARNING RESEARCH</full-title></periodical><pages>1929-1958</pages><volume>15</volume><keywords><keyword>neural networks; regularization; model combination; deep learning</keyword></keywords><dates><year>2014</year></dates><urls/><abstract>Deep neural nets with a large number of parameters are very powerful
machine learning systems. However, overfitting is a serious problem in
such networks. Large networks are also slow to use, making it difficult
to deal with overfitting by combining the predictions of many different
large neural nets at test time. Dropout is a technique for addressing
this problem. The key idea is to randomly drop units (along with their
connections) from the neural network during training. This prevents
units from co-adapting too much. During training, dropout samples from
an exponential number of different ``thinned{''} networks. At test time,
it is easy to approximate the effect of averaging the predictions of all
these thinned networks by simply using a single unthinned network that
has smaller weights. This significantly reduces overfitting and gives
major improvements over other regularization methods. We show that
dropout improves the performance of neural networks on supervised
learning tasks in vision, speech recognition, document classification
and computational biology, obtaining state-of-the-art results on many
benchmark data sets.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Blei, D M</author><author>Ng, A Y</author><author>Jordan, M I</author></authors></contributors><titles><title>Latent Dirichlet allocation</title><secondary-title>JOURNAL OF MACHINE LEARNING RESEARCH</secondary-title></titles><periodical><full-title>JOURNAL OF MACHINE LEARNING RESEARCH</full-title></periodical><pages>993-1022</pages><volume>3</volume><issue>4-5</issue><keywords/><dates><year>2003</year></dates><electronic-resource-num>10.1162/jmlr.2003.3.4-5.993</electronic-resource-num><urls/><abstract>We describe latent Dirichlet allocation (LDA), a generative
probabilistic model for collections of discrete data such as text
corpora. LDA is a three-level hierarchical Bayesian model, in which each
item of a collection is modeled as a finite mixture over an underlying
set of topics. Each topic is, in turn, modeled as an infinite mixture
over an underlying set of topic probabilities. In the context of text
modeling, the topic probabilities provide an explicit representation of
a document. We present efficient approximate inference techniques based
on variational methods and an EM algorithm for empirical Bayes parameter
estimation. We report results in document modeling, text classification,
and collaborative filtering, comparing to a mixture of unigrams model
and the probabilistic LSI model.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>Paszke, Adam</author><author>Gross, Sam</author><author>Massa, Francisco</author><author>Lerer, Adam</author><author>Bradbury, James</author><author>Chanan, Gregory</author><author>Killeen, Trevor</author><author>Lin, Zeming</author><author>Gimelshein, Natalia</author><author>Antiga, Luca</author><author>Desmaison, Alban</author><author>Kopf, Andreas</author><author>Yang, Edward</author><author>DeVito, Zach</author><author>Raison, Martin</author><author>Tejani, Alykhan</author><author>Chilamkurthy, Sasank</author><author>Steiner, Benoit</author><author>Fang, Lu</author><author>Bai, Junjie</author><author>Chintala, Soumith</author></authors><secondary-authors><author>Wallach, H</author><author>Larochelle, H</author><author>Beygelzimer, A</author><author>d'Alche-Buc, F</author><author>Fox, E</author><author>Garnett, R</author></secondary-authors></contributors><titles><title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</title><secondary-title>ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)</secondary-title></titles><periodical><full-title>ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)</full-title></periodical><volume>32</volume><keywords/><dates><year>2019</year></dates><urls/><abstract>Deep learning frameworks have often focused on either usability or
speed, but not both. PyTorch is a machine learning library that shows
that these two goals are in fact compatible: it provides an imperative
and Pythonic programming style that supports code as a model, makes
debugging easy and is consistent with other popular scientific computing
libraries, while remaining efficient and supporting hardware
accelerators such as GPUs.
In this paper, we detail the principles that drove the implementation of
PyTorch and how they are reflected in its architecture. We emphasize
that every aspect of PyTorch is a regular Python program under the full
control of its user. We also explain how the careful and pragmatic
implementation of the key components of its runtime enables them to work
together to achieve compelling performance.
We demonstrate the efficiency of individual subsystems, as well as the
overall speed of PyTorch on several common benchmarks.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>Chen, Tianqi</author><author>Guestrin, Carlos</author></authors></contributors><titles><title>XGBoost: A Scalable Tree Boosting System</title><secondary-title>KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING</secondary-title></titles><periodical><full-title>KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING</full-title></periodical><pages>785-794</pages><keywords><keyword>Large-scale Machine Learning</keyword></keywords><dates><year>2016</year></dates><isbn>978-1-4503-4232-2</isbn><electronic-resource-num>10.1145/2939672.2939785</electronic-resource-num><urls/><abstract>Tree boosting is a highly effective and widely used machine learning
method. In this paper, we describe a scalable end-to-end tree boosting
system called XGBoost, which is used widely by data scientists to
achieve state-of-the-art results on many machine learning challenges. We
propose a novel sparsity-aware algorithm for sparse data and weighted
quantile sketch for approximate tree learning. More importantly, we
provide insights on cache access patterns, data compression and sharding
to build a scalable tree boosting system. By combining these insights,
XGBoost scales beyond billions of examples using far fewer resources
than existing systems.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Jumper, John</author><author>Evans, Richard</author><author>Pritzel, Alexander</author><author>Green, Tim</author><author>Figurnov, Michael</author><author>Ronneberger, Olaf</author><author>Tunyasuvunakool, Kathryn</author><author>Bates, Russ</author><author>Zidek, Augustin</author><author>Potapenko, Anna</author><author>Bridgland, Alex</author><author>Meyer, Clemens</author><author>Kohl, Simon A A</author><author>Ballard, Andrew J</author><author>Cowie, Andrew</author><author>Romera-Paredes, Bernardino</author><author>Nikolov, Stanislav</author><author>Jain, Rishub</author><author>Adler, Jonas</author><author>Back, Trevor</author><author>Petersen, Stig</author><author>Reiman, David</author><author>Clancy, Ellen</author><author>Zielinski, Michal</author><author>Steinegger, Martin</author><author>Pacholska, Michalina</author><author>Berghammer, Tamas</author><author>Bodenstein, Sebastian</author><author>Silver, David</author><author>Vinyals, Oriol</author><author>Senior, Andrew W</author><author>Kavukcuoglu, Koray</author><author>Kohli, Pushmeet</author><author>Hassabis, Demis</author></authors></contributors><titles><title>Highly accurate protein structure prediction with AlphaFold</title><secondary-title>NATURE</secondary-title></titles><periodical><full-title>NATURE</full-title></periodical><pages>583+</pages><volume>596</volume><issue>7873</issue><keywords/><dates><year>2021</year></dates><electronic-resource-num>10.1038/s41586-021-03819-2</electronic-resource-num><urls/><abstract>Proteins are essential to life, and understanding their structure can
facilitate a mechanistic understanding of their function. Through an
enormous experimental effort(1-4), the structures of around 100,000
unique proteins have been determined(5), but this represents a small
fraction of the billions of known protein sequences(6,7). Structural
coverage is bottlenecked by the months to years of painstaking effort
required to determine a single protein structure. Accurate computational
approaches are needed to address this gap and to enable large-scale
structural bioinformatics. Predicting the three-dimensional structure
that a protein will adopt based solely on its amino acid sequence-the
structure prediction component of the `protein folding problem'(8)-has
been an important open research problem for more than 50 years(9).
Despite recent progress(10-14), existing methods fall far short of
atomic accuracy, especially when no homologous structure is available.
Here we provide the first computational method that can regularly
predict protein structures with atomic accuracy even in cases in which
no similar structure is known. We validated an entirely redesigned
version of our neural network-based model, AlphaFold, in the challenging
14th Critical Assessment of protein Structure Prediction (CASP14)(15),
demonstrating accuracy competitive with experimental structures in a
majority of cases and greatly outperforming other methods. Underpinning
the latest version of AlphaFold is a novel machine learning approach
that incorporates physical and biological knowledge about protein
structure, leveraging multi-sequence alignments, into the design of the
deep learning algorithm.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>Szegedy, Christian</author><author>Vanhoucke, Vincent</author><author>Ioffe, Sergey</author><author>Shlens, Jon</author><author>Wojna, Zbigniew</author></authors></contributors><titles><title>Rethinking the Inception Architecture for Computer Vision</title><secondary-title>2016 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)</secondary-title></titles><periodical><full-title>2016 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)</full-title></periodical><pages>2818-2826</pages><keywords/><dates><year>2016</year></dates><isbn>978-1-4673-8851-1</isbn><electronic-resource-num>10.1109/CVPR.2016.308</electronic-resource-num><urls/><abstract>Convolutional networks are at the core of most state-of-the-art computer
vision solutions for a wide variety of tasks. Since 2014 very deep
convolutional networks started to become mainstream, yielding
substantial gains in various benchmarks. Although increased model size
and computational cost tend to translate to immediate quality gains for
most tasks (as long as enough labeled data is provided for training),
computational efficiency and low parameter count are still enabling
factors for various use cases such as mobile vision and big-data
scenarios. Here we are exploring ways to scale up networks in ways that
aim at utilizing the added computation as efficiently as possible by
suitably factorized convolutions and aggressive regularization. We
benchmark our methods on the ILSVRC 2012 classification challenge
validation set demonstrate substantial gains over the state of the art:
21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a
network with a computational cost of 5 billion multiply-adds per
inference and with using less than 25 million parameters. With an
ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5
error and 17.3\% top-1 error on the validation set and 3.6\% top-5 error
on the official test set.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>Abadi, Martin</author><author>Barham, Paul</author><author>Chen, Jianmin</author><author>Chen, Zhifeng</author><author>Davis, Andy</author><author>Dean, Jeffrey</author><author>Devin, Matthieu</author><author>Ghemawat, Sanjay</author><author>Irving, Geoffrey</author><author>Isard, Michael</author><author>Kudlur, Manjunath</author><author>Levenberg, Josh</author><author>Monga, Rajat</author><author>Moore, Sherry</author><author>Murray, Derek G</author><author>Steiner, Benoit</author><author>Tucker, Paul</author><author>Vasudevan, Vijay</author><author>Warden, Pete</author><author>Wicke, Martin</author><author>Yu, Yuan</author><author>Zheng, Xiaoqiang</author></authors></contributors><titles><title>TensorFlow: A system for large-scale machine learning</title><secondary-title>PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION</secondary-title></titles><periodical><full-title>PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION</full-title></periodical><pages>265-283</pages><keywords/><dates><year>2016</year></dates><isbn>978-1-931971-33-1</isbn><urls/><abstract>TensorFlow is a machine learning system that operates at large scale and
in heterogeneous environments. TensorFlow uses dataflow graphs to
represent computation, shared state, and the operations that mutate that
state. It maps the nodes of a dataflow graph across many machines in a
cluster, and within a machine across multiple computational devices,
including multicore CPUs, general-purpose GPUs, and custom-designed
ASICs known as Tensor Processing Units (TPUs). This architecture gives
flexibility to the application developer: whereas in previous
``parameter server{''} designs the management of shared state is built
into the system, TensorFlow enables developers to experiment with novel
optimizations and training algorithms. TensorFlow supports a variety of
applications, with a focus on training and inference on deep neural
networks. Several Google services use TensorFlow in production, we have
released it as an open-source project, and it has become widely used for
machine learning research. In this paper, we describe the TensorFlow
dataflow model and demonstrate the compelling performance that
TensorFlow achieves for several real-world applications.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Pan, Sinno Jialin</author><author>Yang, Qiang</author></authors></contributors><titles><title>A Survey on Transfer Learning</title><secondary-title>IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING</secondary-title></titles><periodical><full-title>IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING</full-title></periodical><pages>1345-1359</pages><volume>22</volume><issue>10</issue><keywords><keyword>Transfer learning; survey; machine learning; data mining</keyword></keywords><dates><year>2010</year></dates><electronic-resource-num>10.1109/TKDE.2009.191</electronic-resource-num><urls/><abstract>A major assumption in many machine learning and data mining algorithms
is that the training and future data must be in the same feature space
and have the same distribution. However, in many real-world
applications, this assumption may not hold. For example, we sometimes
have a classification task in one domain of interest, but we only have
sufficient training data in another domain of interest, where the latter
data may be in a different feature space or follow a different data
distribution. In such cases, knowledge transfer, if done successfully,
would greatly improve the performance of learning by avoiding much
expensive data-labeling efforts. In recent years, transfer learning has
emerged as a new learning framework to address this problem. This survey
focuses on categorizing and reviewing the current progress on transfer
learning for classification, regression, and clustering problems. In
this survey, we discuss the relationship between transfer learning and
other related machine learning techniques such as domain adaptation,
multitask learning and sample selection bias, as well as covariate
shift. We also explore some potential future issues in transfer learning
research.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Fawcett, Tom</author></authors></contributors><titles><title>An introduction to ROC analysis</title><secondary-title>PATTERN RECOGNITION LETTERS</secondary-title></titles><periodical><full-title>PATTERN RECOGNITION LETTERS</full-title></periodical><pages>861-874</pages><volume>27</volume><issue>8</issue><keywords><keyword>ROC analysis; classifier evaluation; evaluation metrics</keyword></keywords><dates><year>2006</year></dates><electronic-resource-num>10.1016/j.patrec.2005.10.010</electronic-resource-num><urls/><abstract>Receiver operating characteristics (ROC) graphs are useful for
organizing classifiers and visualizing their performance. ROC graphs are
commonly used in medical decision making, and in recent years have been
used increasingly in machine learning and data mining research. Although
ROC graphs are apparently simple, there are some common misconceptions
and pitfalls when using them in practice. The purpose of this article is
to serve as an introduction to ROC graphs and as a guide for using them
in research. (c) 2005 Elsevier B.V. All rights reserved.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Phillips, S J</author><author>Anderson, R P</author><author>Schapire, R E</author></authors></contributors><titles><title>Maximum entropy modeling of species geographic distributions</title><secondary-title>ECOLOGICAL MODELLING</secondary-title></titles><periodical><full-title>ECOLOGICAL MODELLING</full-title></periodical><pages>231-259</pages><volume>190</volume><issue>3-4</issue><keywords><keyword>maximum entropy; distribution; modeling; niche; range</keyword></keywords><dates><year>2006</year></dates><electronic-resource-num>10.1016/j.ecolmodel.2005.03.026</electronic-resource-num><urls/><abstract>The availability of detailed environmental data, together with
inexpensive and powerful computers, has fueled a rapid increase in
predictive modeling of species environmental requirements and geographic
distributions. For some species, detailed presence/absence occurrence
data are available, allowing the use of a variety of standard
statistical techniques. However, absence data are not available for most
species. In this paper, we introduce the use of the maximum entropy
method (Maxent) for modeling species geographic distributions with
presence-only data. Maxent is a general-purpose machine learning method
with a simple and precise mathematical formulation, and it has a number
of aspects that make it well-suited for species distribution modeling.
In order to investigate the efficacy of the method, here we perform a
continental-scale case study using two Neotropical mammals: a lowland
species of sloth, Bradypus variegatus, and a small montane murid rodent,
Microryzomys minutus. We compared Maxent predictions with those of a
commonly used presence-only modeling method, the Genetic Algorithm for
Rule-Set Prediction (GARP). We made predictions on 10 random subsets of
the occurrence records for both species, and then used the remaining
localities for testing. Both algorithms provided reasonable estimates of
the species' range, far superior to the shaded outline maps available in
field guides. All models were significantly better than random in both
binomial tests of omission and receiver operating characteristic (ROC)
analyses. The area under the ROC curve (AUC) was almost always higher
for Maxent, indicating better discrimination of suitable versus
unsuitable areas for the species. The Maxent modeling approach can be
used in its present form for many applications with presence-only
datasets, and merits further research and development. (c) 2005 Elsevier
B.V. All rights reserved.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Schmidhuber, Juergen</author></authors></contributors><titles><title>Deep learning in neural networks: An overview</title><secondary-title>NEURAL NETWORKS</secondary-title></titles><periodical><full-title>NEURAL NETWORKS</full-title></periodical><pages>85-117</pages><volume>61</volume><keywords><keyword>Deep learning; Supervised learning; Unsupervised learning; Reinforcement learning; Evolutionary computation</keyword></keywords><dates><year>2015</year></dates><electronic-resource-num>10.1016/j.neunet.2014.09.003</electronic-resource-num><urls/><abstract>In recent years, deep artificial neural networks (including recurrent
ones) have won numerous contests in pattern recognition and machine
learning. This historical survey compactly summarizes relevant work,
much of it from the previous millennium. Shallow and Deep Learners are
distinguished by the depth of their credit assignment paths, which are
chains of possibly learnable, causal links between actions and effects.
I review deep supervised learning (also recapitulating the history of
backpropagation), unsupervised learning, reinforcement learning \&amp;
evolutionary computation, and indirect search for short programs
encoding deep and large networks. (C) 2014 Published by Elsevier Ltd.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Breiman, L</author></authors></contributors><titles><title>Bagging predictors</title><secondary-title>MACHINE LEARNING</secondary-title></titles><periodical><full-title>MACHINE LEARNING</full-title></periodical><pages>123-140</pages><volume>24</volume><issue>2</issue><keywords><keyword>aggregation; bootstrap; averaging; combining</keyword></keywords><dates><year>1996</year></dates><electronic-resource-num>10.1007/bf00058655</electronic-resource-num><urls/><abstract>Bagging predictors is a method for generating multiple versions of a
predictor and using these to gel an aggregated predictor. The
aggregation averages over the versions when predicting a numerical
outcome and does a plurality vote when predicting a class. The multiple
versions are formed by making bootstrap replicates of the learning set
and using these as new learning sets. Tests on real and simulated data
sets using classification and regression trees and subset selection in
linear regression show that bagging can give substantial gains in
accuracy. The vital element is the instability of the prediction method.
If perturbing the learning set can cause significant changes in the
predictor constructed, then bagging can improve accuracy.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Demsar, J</author></authors></contributors><titles><title>Statistical comparisons of classifiers over multiple data sets</title><secondary-title>JOURNAL OF MACHINE LEARNING RESEARCH</secondary-title></titles><periodical><full-title>JOURNAL OF MACHINE LEARNING RESEARCH</full-title></periodical><pages>1-30</pages><volume>7</volume><keywords><keyword>comparative studies; statistical methods; Wilcoxon signed ranks test; Friedman test; multiple comparisons tests</keyword></keywords><dates><year>2006</year></dates><urls/><abstract>While methods for comparing two learning algorithms on a single data set
have been scrutinized for quite some time already, the issue of
statistical tests for comparisons of more algorithms on multiple data
sets, which is even more essential to typical machine learning studies,
has been all but ignored. This article reviews the current practice and
then theoretically and empirically examines several suitable tests.
Based on that, we recommend a set of simple, yet safe and robust
non-parametric tests for statistical comparisons of classifiers: the
Wilcoxon signed ranks test for comparison of two classifiers and the
Friedman test with the corresponding post-hoc tests for comparison of
more classifiers over multiple data sets. Results of the latter can also
be neatly presented with the newly introduced CD ( critical difference)
diagrams.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>Tan, Mingxing</author><author>Le, Quoc V</author></authors><secondary-authors><author>Chaudhuri, K</author><author>Salakhutdinov, R</author></secondary-authors></contributors><titles><title>EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title><secondary-title>INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 97</secondary-title></titles><periodical><full-title>INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 97</full-title></periodical><volume>97</volume><keywords/><dates><year>2019</year></dates><urls/><abstract>Convolutional Neural Networks (ConvNets) are commonly developed at a
fixed resource budget, and then scaled up for better accuracy if more
resources are available. In this paper, we systematically study model
scaling and identify that carefully balancing network depth, width, and
resolution can lead to better performance. Based on this observation, we
propose a new scaling method that uniformly scales all dimensions of
depth/width/resolution using a simple yet highly effective compound
coefficient. We demonstrate the effectiveness of this method on scaling
up MobileNets and ResNet.
To go even further, we use neural architecture search to design a new
baseline network and scale it up to obtain a family of models, called
Efficient-Nets, which achieve much better accuracy and efficiency than
previous ConvNets. In particular, our EfficientNet-B7 achieves
state-of-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while
being 8.4x smaller and 6.1x faster on inference than the best existing
ConvNet. Our EfficientNets also transfer well and achieve
state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3
other transfer learning datasets, with an order of magnitude fewer
parameters.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>Ioffe, Sergey</author><author>Szegedy, Christian</author></authors><secondary-authors><author>Bach, F</author><author>Blei, D</author></secondary-authors></contributors><titles><title>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title><secondary-title>INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 37</secondary-title></titles><periodical><full-title>INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 37</full-title></periodical><pages>448-456</pages><volume>37</volume><keywords/><dates><year>2015</year></dates><urls/><abstract>Training Deep Neural Networks is complicated by the fact that the
distribution of each layer's inputs changes during training, as the
parameters of the previous layers change. This slows down the training
by requiring lower learning rates and careful parameter initialization,
and makes it notoriously hard to train models with saturating
nonlinearities. We refer to this phenomenon as internal covariate shift,
and address the problem by normalizing layer inputs. Our method draws
its strength from making normalization a part of the model architecture
and performing the normalization for each training mini-batch. Batch
Normalization allows us to use much higher learning rates and be less
careful about initialization, and in some cases eliminates the need for
Dropout. Applied to a state-of-the-art image classification model, Batch
Normalization achieves the same accuracy with 14 times fewer training
steps, and beats the original model by a significant margin. Using an
ensemble of batch-normalized networks, we improve upon the best
published result on ImageNet classification: reaching 4.82\% top-5 test
error, exceeding the accuracy of human raters.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>Viola, P</author><author>Jones, M</author></authors><secondary-authors><author>Jacobs, A</author><author>Baldwin, T</author></secondary-authors></contributors><titles><title>Rapid object detection using a boosted cascade of simple features</title><secondary-title>2001 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, VOL 1, PROCEEDINGS</secondary-title></titles><periodical><full-title>2001 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, VOL 1, PROCEEDINGS</full-title></periodical><pages>511-518</pages><keywords/><dates><year>2001</year></dates><isbn>0-7695-1272-0</isbn><electronic-resource-num>10.1109/cvpr.2001.990517</electronic-resource-num><urls/><abstract>This paper describes a machine learning approach for visual object
detection which is capable of processing images extremely rapidly and
achieving high detection rates. This work is distinguished by three key
contributions. The first is the introduction of a new image
representation called the ``Integral Image{''} which allows the features
used by our detector to be computed very quickly. The second is a
learning algorithm, based on AdaBoost, which selects a small number of
critical visual features from a larger set and yields extremely
efficient classifiers{[}5]. The third contribution is a method for
combining increasingly more complex classifiers in a ``cascade{''} which
allows background regions of the image to be quickly discarded while
spending more computation on promising object-like regions. The cascade
can be viewed as an object specific focus-of-attention mechanism which
unlike previous approaches provides statistical guarantees that
discarded regions are unlikely to contain the object of interest. In the
domain of face detection the system yields detection rates comparable to
the best previous systems. Used in real-time applications, the detector
runs at 15 frames per second without resorting to image differencing or
skin color detection.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>WATKINS, CJCH</author><author>DAYAN, P</author></authors></contributors><titles><title>Q-LEARNING</title><secondary-title>MACHINE LEARNING</secondary-title></titles><periodical><full-title>MACHINE LEARNING</full-title></periodical><pages>279-292</pages><volume>8</volume><issue>3-4</issue><keywords><keyword>Q-LEARNING; REINFORCEMENT LEARNING; TEMPORAL DIFFERENCES; ASYNCHRONOUS DYNAMIC PROGRAMMING</keyword></keywords><dates><year>1992</year></dates><electronic-resource-num>10.1023/A:1022676722315</electronic-resource-num><urls/><abstract>Q-learning (Watkins, 1989) is a simple way for agents to learn how to
act optimally in controlled Markovian domains. It amounts to an
incremental method for dynamic programming which imposes limited
computational demands. It works by successively improving its
evaluations of the quality of particular actions at particular states.
This paper presents and proves in detail a convergence theorem for
Q-learning based on that outlined in Watkins (1989). We show that
Q-learning converges to the optimum action-values with probability 1 so
long as all actions are repeatedly sampled in all states and the
action-values are represented discretely. We also sketch extensions to
the cases of non-discounted, but absorbing, Markov environments, and
where many Q values can be changed each iteration, rather than just one.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>DePristo, Mark A</author><author>Banks, Eric</author><author>Poplin, Ryan</author><author>Garimella, Kiran V</author><author>Maguire, Jared R</author><author>Hartl, Christopher</author><author>Philippakis, Anthony A</author><author>del Angel, Guillermo</author><author>Rivas, Manuel A</author><author>Hanna, Matt</author><author>McKenna, Aaron</author><author>Fennell, Tim J</author><author>Kernytsky, Andrew M</author><author>Sivachenko, Andrey Y</author><author>Cibulskis, Kristian</author><author>Gabriel, Stacey B</author><author>Altshuler, David</author><author>Daly, Mark J</author></authors></contributors><titles><title>A framework for variation discovery and genotyping using next-generation DNA sequencing data</title><secondary-title>NATURE GENETICS</secondary-title></titles><periodical><full-title>NATURE GENETICS</full-title></periodical><pages>491+</pages><volume>43</volume><issue>5</issue><keywords/><dates><year>2011</year></dates><electronic-resource-num>10.1038/ng.806</electronic-resource-num><urls/><abstract>Recent advances in sequencing technology make it possible to
comprehensively catalog genetic variation in population samples,
creating a foundation for understanding human disease, ancestry and
evolution. The amounts of raw data produced are prodigious, and many
computational steps are required to translate this output into
high-quality variant calls. We present a unified analytic framework to
discover and genotype variation among multiple samples simultaneously
that achieves sensitive and specific results across five sequencing
technologies and three distinct, canonical experimental designs. Our
process includes (i) initial read mapping; (ii) local realignment around
indels; (iii) base quality score recalibration; (iv) SNP discovery and
genotyping to find all potential variants; and (v) machine learning to
separate true segregating variation from machine artifacts common to
next-generation sequencing technologies. We here discuss the application
of these tools, instantiated in the Genome Analysis Toolkit, to deep
whole-genome, whole-exome capture and multi-sample low-pass (similar to
4x) 1000 Genomes Project datasets.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Haykin, S</author></authors></contributors><titles><title>Cognitive radio: Brain-empowered wireless communications</title><secondary-title>IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS</secondary-title></titles><periodical><full-title>IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS</full-title></periodical><pages>201-220</pages><volume>23</volume><issue>2</issue><keywords><keyword>awareness; channel-state estimation and predictive modeling; cognition; competition and cooperation; emergent behavior; interference temperature; machine learning; radio-scene analysis; rate feedback; spectrum analysis; spectrum holes; spectrum management; stochastic games; transmit-power control; water filling</keyword></keywords><dates><year>2005</year></dates><electronic-resource-num>10.1109/JSAC.2004.839380</electronic-resource-num><urls/><abstract>Cognitive radio is viewed as a novel approach for improving the
utilization of a precious natural resource: the radio electromagnetic
spectrum.
The cognitive radio, built on a software-defined radio, is defined as an
intelligent wireless communication system that is aware of M environment
and uses the methodology of understanding-by-building to learn from the
environment and adapt to statistical variations in the input stimuli,
with two primary objectives in mind:
highly reliable communication whenever and wherever needed;
efficient utilization of the radio spectrum.
Following the discussion of interference temperature as a new metric for
the quantification and management of interference, the paper addresses
three fundamental cognitive tasks.
1) Radio-scene analysis.
2) Channel-state estimation and predictive modeling.
3) Transmit-power control and dynamic spectrum management.
This paper also discusses the emergent behavior of cognitive radio.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Chen, Chengjie</author><author>Chen, Hao</author><author>Zhang, Yi</author><author>Thomas, Hannah R</author><author>Frank, Margaret H</author><author>He, Yehua</author><author>Xia, Rui</author></authors></contributors><titles><title>TBtools: An Integrative Toolkit Developed for Interactive Analyses of Big Biological Data</title><secondary-title>MOLECULAR PLANT</secondary-title></titles><periodical><full-title>MOLECULAR PLANT</full-title></periodical><pages>1194-1202</pages><volume>13</volume><issue>8</issue><keywords><keyword>TBtools; bioinformatics; big data; data visulization; gene family</keyword></keywords><dates><year>2020</year></dates><electronic-resource-num>10.1016/j.molp.2020.06.009</electronic-resource-num><urls/><abstract>The rapid development of high-throughput sequencing techniques has led
biology into the big-data era. Data analyses using various
bioinformatics tools rely on programming and command-line environments,
which are challenging and time-consuming for most wet-lab biologists.
Here, we present TBtools (a Toolkit for Biologists integrating various
biological data-handling tools), a stand-alone software with a
user-friendly interface. The toolkit incorporates over 130 functions,
which are designed to meet the increasing demand for big-data analyses,
ranging from bulk sequence processing to interactive data visualization.
A wide variety of graphs can be prepared in TBtools using a new plotting
engine ({''}JIGplot{''}) developed to maximize their interactive
ability; this engine allows quick point-and-click modification of almost
every graphic feature. TBtools is platform-independent software that can
be run under all operating systems with Java Runtime Environment 1.6 or
newer. It is freely available to non-commercial users at
https://github.com/CJ-Chen/TBtools/releases.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Bengio, Yoshua</author><author>Courville, Aaron</author><author>Vincent, Pascal</author></authors></contributors><titles><title>Representation Learning: A Review and New Perspectives</title><secondary-title>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</secondary-title></titles><periodical><full-title>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</full-title></periodical><pages>1798-1828</pages><volume>35</volume><issue>8</issue><keywords><keyword>Deep learning; representation learning; feature learning; unsupervised learning; Boltzmann machine; autoencoder; neural nets</keyword></keywords><dates><year>2013</year></dates><electronic-resource-num>10.1109/TPAMI.2013.50</electronic-resource-num><urls/><abstract>The success of machine learning algorithms generally depends on data
representation, and we hypothesize that this is because different
representations can entangle and hide more or less the different
explanatory factors of variation behind the data. Although specific
domain knowledge can be used to help design representations, learning
with generic priors can also be used, and the quest for AI is motivating
the design of more powerful representation-learning algorithms
implementing such priors. This paper reviews recent work in the area of
unsupervised feature learning and deep learning, covering advances in
probabilistic models, autoencoders, manifold learning, and deep
networks. This motivates longer term unanswered questions about the
appropriate objectives for learning good representations, for computing
representations (i.e., inference), and the geometrical connections
between representation learning, density estimation, and manifold
learning.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Smola, A J</author><author>Schlkopf, B</author></authors></contributors><titles><title>A tutorial on support vector regression</title><secondary-title>STATISTICS AND COMPUTING</secondary-title></titles><periodical><full-title>STATISTICS AND COMPUTING</full-title></periodical><pages>199-222</pages><volume>14</volume><issue>3</issue><keywords><keyword>machine learning; support vector machines; regression estimation</keyword></keywords><dates><year>2004</year></dates><electronic-resource-num>10.1023/B:STCO.0000035301.49549.88</electronic-resource-num><urls/><abstract>In this tutorial we give an overview of the basic ideas underlying
Support Vector (SV) machines for function estimation. Furthermore, we
include a summary of currently used algorithms for training SV machines,
covering both the quadratic (or convex) programming part and advanced
methods for dealing with large datasets. Finally, we mention some
modifications and extensions that have been applied to the standard SV
algorithm, and discuss the aspect of regularization from a SV
perspective.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>Ribeiro, Marco Tulio</author><author>Singh, Sameer</author><author>Guestrin, Carlos</author></authors></contributors><titles><title>``Why Should I Trust You?{''} Explaining the Predictions of Any Classifier</title><secondary-title>KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING</secondary-title></titles><periodical><full-title>KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING</full-title></periodical><pages>1135-1144</pages><keywords/><dates><year>2016</year></dates><isbn>978-1-4503-4232-2</isbn><electronic-resource-num>10.1145/2939672.2939778</electronic-resource-num><urls/><abstract>Despite widespread adoption, machine learning models remain mostly black
boxes. Understanding the reasons behind predictions is, however, quite
important in assessing trust, which is fundamental if one plans to take
action based on a prediction, or when choosing whether to deploy a new
model. Such understanding also provides insights into the model, which
can be used to transform an untrustworthy model or prediction into a
trustworthy one.
In this work, we propose LIME, a novel explanation technique that
explains the predictions of any classifier in an interpretable and
faithful manner, by learning an interpretable model locally around the
prediction. We also propose a method to explain models by presenting
representative individual predictions and their explanations in a
non-redundant way, framing the task as a submodular optimization
problem. We demonstrate the flexibility of these methods by explaining
different models for text (e.g. random forests) and image classification
(e.g. neural networks). We show the utility of explanations via novel
experiments, both simulated and with human subjects, on various
scenarios that require trust: deciding if one should trust a prediction,
choosing between models, improving an untrustworthy classifier, and
identifying why a classifier should not be trusted.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Tamura, Koichiro</author><author>Stecher, Glen</author><author>Kumar, Sudhir</author></authors></contributors><titles><title>MEGA11 Molecular Evolutionary Genetics Analysis Version 11</title><secondary-title>MOLECULAR BIOLOGY AND EVOLUTION</secondary-title></titles><periodical><full-title>MOLECULAR BIOLOGY AND EVOLUTION</full-title></periodical><pages>3022-3027</pages><volume>38</volume><issue>7</issue><keywords><keyword>software; phylogenetics; timetrees; tip dating; neutrality</keyword></keywords><dates><year>2021</year></dates><electronic-resource-num>10.1093/molbev/msab120</electronic-resource-num><urls/><abstract>The Molecular Evolutionary Genetics Analysis (MEGA) software has matured
to contain a large collection of methods and tools of computational
molecular evolution. Here, we describe new additions that make MEGA a
more comprehensive tool for building timetrees of species, pathogens,
and gene families using rapid relaxed-clock methods. Methods for
estimating divergence times and confidence intervals are implemented to
use probability densities for calibration constraints for node-dating
and sequence sampling dates for tip-dating analyses. They are supported
by new options for tagging sequences with spatiotemporal sampling
information, an expanded interactive Node Calibrations Editor, and an
extended Tree Explorer to display timetrees. Also added is a Bayesian
method for estimating neutral evolutionary probabilities of alleles in a
species using multispecies sequence alignments and a machine learning
method to test for the autocorrelation of evolutionary rates in
phylogenies. The computer memory requirements for the maximum likelihood
analysis are reduced significantly through reprogramming, and the
graphical user interface has been made more responsive and interactive
for very big data sets. These enhancements will improve the user
experience, quality of results, and the pace of biological discovery.
Natively compiled graphical user interface and command-line versions of
MEGA11 are available for Microsoft Windows, Linux, and macOS from
www.megasoftware.net.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>Jia, Yangqing</author><author>Shelhamer, Evan</author><author>Donahue, Jeff</author><author>Karayev, Sergey</author><author>Long, Jonathan</author><author>Girshick, Ross</author><author>Guadarrama, Sergio</author><author>Darrell, Trevor</author></authors></contributors><titles><title>Caffe: Convolutional Architecture for Fast Feature Embedding</title><secondary-title>PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14)</secondary-title></titles><periodical><full-title>PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14)</full-title></periodical><pages>675-678</pages><keywords><keyword>Open Source; Computer Vision; Neural Networks; Parallel Computation; Machine Learning</keyword></keywords><dates><year>2014</year></dates><isbn>978-1-4503-3063-3</isbn><electronic-resource-num>10.1145/2647868.2654889</electronic-resource-num><urls/><abstract>Caffe provides multimedia scientists and practitioners with a clean and
modifiable framework for state-of-the-art deep learning algorithms and a
collection of reference models. The framework is a BSD-licensed C++
library with Python and MATLAB bindings for training and deploying
general-purpose convolutional neural networks and other deep models
efficiently on commodity architectures. Caffe fits industry and
internet-scale media needs by CUDA GPU computation, processing over 40
million images a day on a single K40 or Titan GPU (approximate to 2.5 ms
per image). By separating model representation from actual
implementation, Gaffe allows experimentation and seamless switching
among platforms for ease of development and deployment from prototyping
machines to cloud environments.
Caffe is maintained and developed by the Berkeley Vision and Learning
Center (BVLC) with the help of an active community of contributors on
GitHub. It powers ongoing research projects, large-scale industrial
applications, and startup prototypes in vision, speech, and multimedia.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Everingham, Mark</author><author>Van Gool, Luc</author><author>Williams, Christopher K I</author><author>Winn, John</author><author>Zisserman, Andrew</author></authors></contributors><titles><title>The Pascal Visual Object Classes (VOC) Challenge</title><secondary-title>INTERNATIONAL JOURNAL OF COMPUTER VISION</secondary-title></titles><periodical><full-title>INTERNATIONAL JOURNAL OF COMPUTER VISION</full-title></periodical><pages>303-338</pages><volume>88</volume><issue>2, SI</issue><keywords><keyword>Database; Benchmark; Object recognition; Object detection</keyword></keywords><dates><year>2010</year></dates><electronic-resource-num>10.1007/s11263-009-0275-4</electronic-resource-num><urls/><abstract>The Pascal Visual Object Classes (VOC) challenge is a benchmark in
visual object category recognition and detection, providing the vision
and machine learning communities with a standard dataset of images and
annotation, and standard evaluation procedures. Organised annually from
2005 to present, the challenge and its associated dataset has become
accepted as the benchmark for object detection.
This paper describes the dataset and evaluation procedure. We review the
state-of-the-art in evaluated methods for both classification and
detection, analyse whether the methods are statistically different, what
they are learning from the images (e.g. the object or its context), and
what the methods find easy or confuse. The paper concludes with lessons
learnt in the three year history of the challenge, and proposes
directions for future improvement and extension.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Gorelick, Noel</author><author>Hancher, Matt</author><author>Dixon, Mike</author><author>Ilyushchenko, Simon</author><author>Thau, David</author><author>Moore, Rebecca</author></authors></contributors><titles><title>Google Earth Engine: Planetary-scale geospatial analysis for everyone</title><secondary-title>REMOTE SENSING OF ENVIRONMENT</secondary-title></titles><periodical><full-title>REMOTE SENSING OF ENVIRONMENT</full-title></periodical><pages>18-27</pages><volume>202</volume><keywords><keyword>Cloud computing; Big data; Analysis; Platform; Data democratization; Earth Engine</keyword></keywords><dates><year>2017</year></dates><electronic-resource-num>10.1016/j.rse.2017.06.031</electronic-resource-num><urls/><abstract>Google Earth Engine is a cloud-based platform for planetary-scale
geospatial analysis that brings Google's massive computational
capabilities to bear on a variety of high-impact societal issues
including deforestation, drought, disaster, disease, food security,
water management, climate monitoring and environmental protection. It is
unique in the field as an integrated platform designed to empower not
only traditional remote sensing scientists, but also a much wider
audience that lacks the technical capacity needed to utilize traditional
supercomputers or large-scale commodity cloud computing resources. (C)
2017 The Author(s). Published by Elsevier Inc.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>McMahan, H Brendan</author><author>Moore, Eider</author><author>Ramage, Daniel</author><author>Hampson, Seth</author><author>y Arcas, Blaise</author></authors><secondary-authors><author>Singh, A</author><author>Zhu, J</author></secondary-authors></contributors><titles><title>Communication-Efficient Learning of Deep Networks from Decentralized Data</title><secondary-title>ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54</secondary-title></titles><periodical><full-title>ARTIFICIAL INTELLIGENCE AND STATISTICS, VOL 54</full-title></periodical><pages>1273-1282</pages><volume>54</volume><keywords/><dates><year>2017</year></dates><urls/><abstract>Modern mobile devices have access to a wealth of data suitable for
learning models, which in turn can greatly improve the user experience
on the device. For example, language models can improve speech
recognition and text entry, and image models can automatically select
good photos. However, this rich data is often privacy sensitive, large
in quantity, or both, which may preclude logging to the data center and
training there using conventional approaches. We advocate an alternative
that leaves the training data distributed on the mobile devices, and
learns a shared model by aggregating locally-computed updates. We term
this decentralized approach Federated Learning.
We present a practical method for the federated learning of deep
networks based on iterative model averaging, and conduct an extensive
empirical evaluation, considering five different model architectures and
four datasets. These experiments demonstrate the approach is robust to
the unbalanced and non-IID data distributions that are a defining
characteristic of this setting. Communication costs are the principal
constraint, and we show a reduction in required communication rounds by
10-100x as compared to synchronized stochastic gradient descent.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Tang, Zefang</author><author>Li, Chenwei</author><author>Kang, Boxi</author><author>Gao, Ge</author><author>Li, Cheng</author><author>Zhang, Zemin</author></authors></contributors><titles><title>GEPIA: a web server for cancer and normal gene expression profiling and interactive analyses</title><secondary-title>NUCLEIC ACIDS RESEARCH</secondary-title></titles><periodical><full-title>NUCLEIC ACIDS RESEARCH</full-title></periodical><pages>W98-W102</pages><volume>45</volume><issue>W1</issue><keywords/><dates><year>2017</year></dates><electronic-resource-num>10.1093/nar/gkx247</electronic-resource-num><urls/><abstract>Tremendous amount of RNA sequencing data have been produced by large
consortium projects such as TCGA and GTEx, creating new opportunities
for data mining and deeper understanding of gene functions. While
certain existing web servers are valuable and widely used, many
expression analysis functions needed by experimental biologists are
still not adequately addressed by these tools. We introduce GEPIA (Gene
Expression Profiling Interactive Analysis), a web-based tool to deliver
fast and customizable functionalities based on TCGA and GTEx data. GEPIA
provides key interactive and customizable functions including
differential expression analysis, profiling plotting, correlation
analysis, patient survival analysis, similar gene detection and
dimensionality reduction analysis. The comprehensive expression analyses
with simple clicking through GEPIA greatly facilitate data mining in
wide research areas, scientific discussion and the therapeutic discovery
process. GEPIA fills in the gap between cancer genomics big data and the
delivery of integrated information to end users, thus helping unleash
the value of the current data resources. GEPIA is available at
http://gepia.cancer-pku.cn/.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Elith, J</author><author>Graham, C H</author><author>Anderson, R P</author><author>Dudk, M</author><author>Ferrier, S</author><author>Guisan, A</author><author>Hijmans, R J</author><author>Huettmann, F</author><author>Leathwick, J R</author><author>Lehmann, A</author><author>Li, J</author><author>Lohmann, L G</author><author>Loiselle, B A</author><author>Manion, G</author><author>Moritz, C</author><author>Nakamura, M</author><author>Nakazawa, Y</author><author>Overton, J M</author><author>Peterson, A T</author><author>Phillips, S J</author><author>Richardson, K</author><author>Scachetti-Pereira, R</author><author>Schapire, R E</author><author>Sobern, J</author><author>Williams, S</author><author>Wisz, M S</author><author>Zimmermann, N E</author></authors></contributors><titles><title>Novel methods improve prediction of species' distributions from occurrence data</title><secondary-title>ECOGRAPHY</secondary-title></titles><periodical><full-title>ECOGRAPHY</full-title></periodical><pages>129-151</pages><volume>29</volume><issue>2</issue><keywords/><dates><year>2006</year></dates><electronic-resource-num>10.1111/j.2006.0906-7590.04596.x</electronic-resource-num><urls/><abstract>Prediction of species' distributions is central to diverse applications
in ecology, evolution and conservation science. There is increasing
electronic access to vast sets of occurrence records in museums and
herbaria, yet little effective guidance on how best to use this
information in the context of numerous approaches for modelling
distributions. To meet this need, we compared 16 modelling methods over
226 species from 6 regions of the world, creating the most comprehensive
set of model comparisons to date. We used presence-only data to fit
models, and independent presence-absence data to evaluate the
predictions. Along with well-established modelling methods such as
generalised additive models and GARP and BIOCLIM, we explored methods
that either have been developed recently or have rarely been applied to
modelling species' distributions. These include machine-learning methods
and community models, both of which have features that may make them
particularly well suited to noisy or sparse information, as is typical
of species' occurrence data. Presence-only data were effective for
modelling species' distributions for many species and regions. The novel
methods consistently outperformed more established methods. The results
of our analysis are promising for the use of data from museums and
herbaria, especially as methods suited to the noise inherent in such
data improve.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Guyon, I</author><author>Weston, J</author><author>Barnhill, S</author><author>Vapnik, V</author></authors></contributors><titles><title>Gene selection for cancer classification using support vector machines</title><secondary-title>MACHINE LEARNING</secondary-title></titles><periodical><full-title>MACHINE LEARNING</full-title></periodical><pages>389-422</pages><volume>46</volume><issue>1-3</issue><keywords><keyword>diagnosis; diagnostic tests; drug discovery; RNA expression; genomics; gene selection; DNA micro-array; proteomics; cancer classification; feature selection; support vector machines; recursive feature elimination</keyword></keywords><dates><year>2002</year></dates><electronic-resource-num>10.1023/A:1012487302797</electronic-resource-num><urls/><abstract>DNA micro-arrays now permit scientists to screen thousands of genes
simultaneously and determine whether those genes are active, hyperactive
or silent in normal or cancerous tissue. Because these new micro-array
devices generate bewildering amounts of raw data, new analytical methods
must be developed to sort out whether cancer tissues have distinctive
signatures of gene expression over normal tissues or other types of
cancer tissues.
In this paper, we address the problem of selection of a small subset of
genes from broad patterns of gene expression data, recorded on DNA
micro-arrays. Using available training examples from cancer and normal
patients, we build a classifier suitable for genetic diagnosis, as well
as drug discovery. Previous attempts to address this problem select
genes with correlation techniques. We propose a new method of gene
selection utilizing Support Vector Machine methods based on Recursive
Feature Elimination (RFE). We demonstrate experimentally that the genes
selected by our techniques yield better classification performance and
are biologically relevant to cancer.
In contrast with the baseline method, our method eliminates gene
redundancy automatically and yields better and more compact gene
subsets. In patients with leukemia our method discovered 2 genes that
yield zero leave-one-out error, while 64 genes are necessary for the
baseline method to get the best result (one leave-one-out error). In the
colon cancer database, using only 4 genes our method is 98\% accurate,
while the baseline method is only 86\% accurate.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Dormann, Carsten F</author><author>Elith, Jane</author><author>Bacher, Sven</author><author>Buchmann, Carsten</author><author>Carl, Gudrun</author><author>Carre, Gabriel</author><author>Garcia Marquez, Jaime R</author><author>Gruber, Bernd</author><author>Lafourcade, Bruno</author><author>Leitao, Pedro J</author><author>Muenkemueller, Tamara</author><author>McClean, Colin</author><author>Osborne, Patrick E</author><author>Reineking, Bjoern</author><author>Schroeder, Boris</author><author>Skidmore, Andrew K</author><author>Zurell, Damaris</author><author>Lautenbach, Sven</author></authors></contributors><titles><title>Collinearity: a review of methods to deal with it and a simulation study evaluating their performance</title><secondary-title>ECOGRAPHY</secondary-title></titles><periodical><full-title>ECOGRAPHY</full-title></periodical><pages>27-46</pages><volume>36</volume><issue>1</issue><keywords/><dates><year>2013</year></dates><electronic-resource-num>10.1111/j.1600-0587.2012.07348.x</electronic-resource-num><urls/><abstract>Collinearity refers to the non independence of predictor variables,
usually in a regression-type analysis. It is a common feature of any
descriptive ecological data set and can be a problem for parameter
estimation because it inflates the variance of regression parameters and
hence potentially leads to the wrong identification of relevant
predictors in a statistical model. Collinearity is a severe problem when
a model is trained on data from one region or time, and predicted to
another with a different or unknown structure of collinearity. To
demonstrate the reach of the problem of collinearity in ecology, we show
how relationships among predictors differ between biomes, change over
spatial scales and through time. Across disciplines, different
approaches to addressing collinearity problems have been developed,
ranging from clustering of predictors, threshold-based pre-selection,
through latent variable methods, to shrinkage and regularisation. Using
simulated data with five predictor-response relationships of increasing
complexity and eight levels of collinearity we compared ways to address
collinearity with standard multiple regression and machine-learning
approaches. We assessed the performance of each approach by testing its
impact on prediction to new data. In the extreme, we tested whether the
methods were able to identify the true underlying relationship in a
training dataset with strong collinearity by evaluating its performance
on a test dataset without any collinearity. We found that methods
specifically designed for collinearity, such as latent variable methods
and tree based models, did not outperform the traditional GLM and
threshold-based pre-selection. Our results highlight the value of GLM in
combination with penalised methods (particularly ridge) and
threshold-based pre-selection when omitted variables are considered in
the final interpretation. However, all approaches tested yielded
degraded predictions under change in collinearity structure and the folk
lore'-thresholds of correlation coefficients between predictor variables
of |r| &gt;0.7 was an appropriate indicator for when collinearity begins to
severely distort model estimation and subsequent prediction. The use of
ecological understanding of the system in pre-analysis variable
selection and the choice of the least sensitive statistical approaches
reduce the problems of collinearity, but cannot ultimately solve them.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Zhou, Yingyao</author><author>Zhou, Bin</author><author>Pache, Lars</author><author>Chang, Max</author><author>Khodabakhshi, Alireza Hadj</author><author>Tanaseichuk, Olga</author><author>Benner, Christopher</author><author>Chanda, Sumit K</author></authors></contributors><titles><title>Metascape provides a biologist-oriented resource for the analysis of systems-level datasets</title><secondary-title>NATURE COMMUNICATIONS</secondary-title></titles><periodical><full-title>NATURE COMMUNICATIONS</full-title></periodical><volume>10</volume><keywords/><dates><year>2019</year></dates><electronic-resource-num>10.1038/s41467-019-09234-6</electronic-resource-num><urls/><abstract>A critical component in the interpretation of systems-level studies is
the inference of enriched biological pathways and protein complexes
contained within OMICs datasets. Successful analysis requires the
integration of a broad set of current biological databases and the
application of a robust analytical pipeline to produce readily
interpretable results. Metascape is a web-based portal designed to
provide a comprehensive gene list annotation and analysis resource for
experimental biologists. In terms of design features, Metascape combines
functional enrichment, interactome analysis, gene annotation, and
membership search to leverage over 40 independent knowledgebases within
one integrated portal. Additionally, it facilitates comparative analyses
of datasets across multiple independent and orthogonal experiments.
Metascape provides a significantly simplified user experience through a
one-click Express Analysis interface to generate interpretable outputs.
Taken together, Metascape is an effective and efficient tool for
experimental biologists to comprehensively analyze and interpret
OMICs-based studies in the big data era.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>Ke, Guolin</author><author>Meng, Qi</author><author>Finley, Thomas</author><author>Wang, Taifeng</author><author>Chen, Wei</author><author>Ma, Weidong</author><author>Ye, Qiwei</author><author>Liu, Tie-Yan</author></authors><secondary-authors><author>Guyon, I</author><author>Luxburg, U V</author><author>Bengio, S</author><author>Wallach, H</author><author>Fergus, R</author><author>Vishwanathan, S</author><author>Garnett, R</author></secondary-authors></contributors><titles><title>LightGBM: A Highly Efficient Gradient Boosting Decision Tree</title><secondary-title>ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)</secondary-title></titles><periodical><full-title>ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)</full-title></periodical><volume>30</volume><keywords/><dates><year>2017</year></dates><urls/><abstract>Gradient Boosting Decision Tree (GBDT) is a popular machine learning
algorithm, and has quite a few effective implementations such as XGBoost
and pGBRT. Although many engineering optimizations have been adopted in
these implementations, the efficiency and scalability are still
unsatisfactory when the feature dimension is high and data size is
large. A major reason is that for each feature, they need to scan all
the data instances to estimate the information gain of all possible
split points, which is very time consuming To tackle this problem, we
propose two novel techniques: Gradient-based One-Side Sampling (GOSS)
and Exclusive Feature Bundling (EFB). With GOSS, we exclude a
significant proportion of data instances with small gradients, and only
use the rest to estimate the information gain. We prove that, since the
data instances with larger gradients play a more important role in the
computation of information gain, GOSS can obtain quite accurate
estimation of the information gain with a much smaller data size. With
EFB, we bundle mutually exclusive features (i.e., they rarely take
nonzero values simultaneously), to reduce the number of features. We
prove that finding the optimal bundling of exclusive features is
NP-hard, but a greedy algorithm can achieve quite good approximation
ratio (and thus can effectively reduce the number of features without
hurting the accuracy of split point determination by much). We call our
new GBDT implementation with GOSS and EFB LightGBM. Our experiments on
multiple public datasets show that, LightGBM speeds up the training
process of conventional GBDT by up to over 20 times while achieving
almost the same accuracy.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Bergstra, James</author><author>Bengio, Yoshua</author></authors></contributors><titles><title>Random Search for Hyper-Parameter Optimization</title><secondary-title>JOURNAL OF MACHINE LEARNING RESEARCH</secondary-title></titles><periodical><full-title>JOURNAL OF MACHINE LEARNING RESEARCH</full-title></periodical><pages>281-305</pages><volume>13</volume><keywords><keyword>global optimization; model selection; neural networks; deep learning; response surface modeling</keyword></keywords><dates><year>2012</year></dates><urls/><abstract>Grid search and manual search are the most widely used strategies for
hyper-parameter optimization. This paper shows empirically and
theoretically that randomly chosen trials are more efficient for
hyper-parameter optimization than trials on a grid. Empirical evidence
comes from a comparison with a large previous study that used grid
search and manual search to configure neural networks and deep belief
networks. Compared with neural networks configured by a pure grid
search, we find that random search over the same domain is able to find
models that are as good or better within a small fraction of the
computation time. Granting random search the same computational budget,
random search finds better models by effectively searching a larger,
less promising configuration space. Compared with deep belief networks
configured by a thoughtful combination of manual search and grid search,
purely random search over the same 32-dimensional configuration space
found statistically equal performance on four of seven data sets, and
superior performance on one of seven. A Gaussian process analysis of the
function from hyper-parameters to validation set performance reveals
that for most data sets only a few of the hyper-parameters really
matter, but that different hyper-parameters are important on different
data sets. This phenomenon makes grid search a poor choice for
configuring algorithms for new data sets. Our analysis casts some light
on why recent ``High Throughput{''} methods achieve surprising
success-they appear to search through a large number of hyper-parameters
because most hyper-parameters do not matter much. We anticipate that
growing interest in large hierarchical models will place an increasing
burden on techniques for hyper-parameter optimization; this work shows
that random search is a natural baseline against which to judge progress
in the development of adaptive (sequential) hyper-parameter optimization
algorithms.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>Finn, Chelsea</author><author>Abbeel, Pieter</author><author>Levine, Sergey</author></authors><secondary-authors><author>Precup, D</author><author>Teh, Y W</author></secondary-authors></contributors><titles><title>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title><secondary-title>INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 70</secondary-title></titles><periodical><full-title>INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 70</full-title></periodical><volume>70</volume><keywords/><dates><year>2017</year></dates><urls/><abstract>We propose an algorithm for meta-learning that is model-agnostic, in the
sense that it is compatible with any model trained with gradient descent
and applicable to a variety of different learning problems, including
classification, regression, and reinforcement learning. The goal of
meta-learning is to train a model on a variety of learning tasks, such
that it can solve new learning tasks using only a small number of
training samples. In our approach, the parameters of the model are
explicitly trained such that a small number of gradient steps with a
small amount of training data from a new task will produce good
generalization performance on that task. In effect, our method trains
the model to be easy to fine-tune. We demonstrate that this approach
leads to state-of-the-art performance on two fewshot image
classification benchmarks, produces good results on few-shot regression,
and accelerates fine-tuning for policy gradient reinforcement learning
with neural network policies.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>Arjovsky, Martin</author><author>Chintala, Soumith</author><author>Bottou, Leon</author></authors><secondary-authors><author>Precup, D</author><author>Teh, Y W</author></secondary-authors></contributors><titles><title>Wasserstein Generative Adversarial Networks</title><secondary-title>INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 70</secondary-title></titles><periodical><full-title>INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 70</full-title></periodical><volume>70</volume><keywords/><dates><year>2017</year></dates><urls/><abstract>We introduce a new algorithm named WGAN, an alternative to traditional
GAN training In this new model, we show that we can improve the
stability of learning, get rid of problems like mode collapse, and
provide meaningful learning curves useful for debugging and
hyperparameter searches. Furthermore, we show that the corresponding
optimization problem is sound, and provide extensive theoretical work
highlighting the deep connections to different distances between
distributions.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Duchi, John</author><author>Hazan, Elad</author><author>Singer, Yoram</author></authors></contributors><titles><title>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title><secondary-title>JOURNAL OF MACHINE LEARNING RESEARCH</secondary-title></titles><periodical><full-title>JOURNAL OF MACHINE LEARNING RESEARCH</full-title></periodical><pages>2121-2159</pages><volume>12</volume><keywords><keyword>subgradient methods; adaptivity; online learning; stochastic convex optimization</keyword></keywords><dates><year>2011</year></dates><urls/><abstract>We present a new family of subgradient methods that dynamically
incorporate knowledge of the geometry of the data observed in earlier
iterations to perform more informative gradient-based learning.
Metaphorically, the adaptation allows us to find needles in haystacks in
the form of very predictive but rarely seen features. Our paradigm stems
from recent advances in stochastic optimization and online learning
which employ proximal functions to control the gradient steps of the
algorithm. We describe and analyze an apparatus for adaptively modifying
the proximal function, which significantly simplifies setting a learning
rate and results in regret guarantees that are provably as good as the
best proximal function that can be chosen in hindsight. We give several
efficient algorithms for empirical risk minimization problems with
common and important regularization functions and domain constraints. We
experimentally study our theoretical analysis and show that adaptive
subgradient methods outperform state-of-the-art, yet non-adaptive,
subgradient algorithms.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Book Section">7</ref-type><contributors><authors><author>Xu, Kelvin</author><author>Ba, Jimmy Lei</author><author>Kiros, Ryan</author><author>Cho, Kyunghyun</author><author>Courville, Aaron</author><author>Salakhutdinov, Ruslan</author><author>Zemel, Richard S</author><author>Bengio, Yoshua</author></authors><secondary-authors><author>Bach, F</author><author>Blei, D</author></secondary-authors></contributors><titles><title>Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title><secondary-title>INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 37</secondary-title></titles><periodical><full-title>INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 37</full-title></periodical><pages>2048-2057</pages><volume>37</volume><keywords/><dates><year>2015</year></dates><urls/><abstract>Inspired by recent work in machine translation and object detection, we
introduce an attention based model that automatically learns to describe
the content of images. We describe how we can train this model in a
deterministic manner using standard backpropagation techniques and
stochastically by maximizing a variational lower bound. We also show
through visualization how the model is able to automatically learn to
fix its gaze on salient objects while generating the corresponding words
in the output sequence. We validate the use of attention with
state-of-the-art performance on three benchmark datasets: Flickr9k,
Flickr3Ok and MS COCO.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Rozas, J</author><author>Snchez-DelBarrio, J C</author><author>Messeguer, X</author><author>Rozas, R</author></authors></contributors><titles><title>DnaSP, DNA polymorphism analyses by the coalescent and other methods</title><secondary-title>BIOINFORMATICS</secondary-title></titles><periodical><full-title>BIOINFORMATICS</full-title></periodical><pages>2496-2497</pages><volume>19</volume><issue>18</issue><keywords/><dates><year>2003</year></dates><electronic-resource-num>10.1093/bioinformatics/btg359</electronic-resource-num><urls/><abstract>DnaSP is a software package for the analysis of DNA polymorphism data.
Present version introduces several new modules and features which, among
other options allow: (1) handling big data sets (similar to5 Mb per
sequence); (2) conducting a large number of coalescent-based tests by
Monte Carlo computer simulations; (3) extensive analyses of the genetic
differentiation and gene flow among populations; (4) analysing the
evolutionary pattern of preferred and unpreferred codons; (5) generating
graphical outputs for an easy visualization of results.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Raissi, M</author><author>Perdikaris, P</author><author>Karniadakis, G E</author></authors></contributors><titles><title>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title><secondary-title>JOURNAL OF COMPUTATIONAL PHYSICS</secondary-title></titles><periodical><full-title>JOURNAL OF COMPUTATIONAL PHYSICS</full-title></periodical><pages>686-707</pages><volume>378</volume><keywords><keyword>Data-driven scientific computing; Machine learning; Predictive modeling; Runge-Kutta methods; Nonlinear dynamics</keyword></keywords><dates><year>2019</year></dates><electronic-resource-num>10.1016/j.jcp.2018.10.045</electronic-resource-num><urls/><abstract>We introduce physics-informed neural networks - neural networks that are
trained to solve supervised learning tasks while respecting any given
laws of physics described by general nonlinear partial differential
equations. In this work, we present our developments in the context of
solving two main classes of problems: data-driven solution and
data-driven discovery of partial differential equations. Depending on
the nature and arrangement of the available data, we devise two distinct
types of algorithms, namely continuous time and discrete time models.
The first type of models forms a new family of data-efficient
spatio-temporal function approximators, while the latter type allows the
use of arbitrarily accurate implicit Runge-Kutta time stepping schemes
with unlimited number of stages. The effectiveness of the proposed
framework is demonstrated through a collection of classical problems in
fluids, quantum mechanics, reaction-diffusion systems, and the
propagation of nonlinear shallow-water waves. (C) 2018 Elsevier Inc. All
rights reserved.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Prasad, Nadipuram R</author><author>Almanza-Garcia, Salvador</author><author>Lu, Thomas T</author></authors></contributors><titles><title>Anomaly Detection</title><secondary-title>CMC-COMPUTERS MATERIALS \&amp; CONTINUA</secondary-title></titles><periodical><full-title>CMC-COMPUTERS MATERIALS \&amp; CONTINUA</full-title></periodical><pages>1-22</pages><volume>14</volume><issue>1</issue><keywords><keyword>Anomaly detection; soft-computing; decision-making; machine intelligence; nonlinear dynamical systems</keyword></keywords><dates><year>2009</year></dates><urls/><abstract>The paper presents a revolutionary framework for the modeling,
detection, characterization, identification, and machine-learning of
anomalous behavior in observed phenomena arising from a large class of
unknown and uncertain dynamical systems. An evolved behavior would in
general be very difficult to correct unless the specific anomalous event
that caused such behavior can be detected early, and any consequence
attributed to the specific anomaly following its detection. Substantial
investigative time and effort is required to back-track the cause for
abnormal behavior and to recreate the event sequence leading to such
abnormal behavior. The need to automatically detect anomalous behavior
is therefore critical using principles of state motion, and to do so
with a human operator in the loop. Human-machine interaction results in
a capability for machine self-learning and in producing a robust
decision-support mechanism. This is the fundamental concept of
intelligent control wherein machine-learning is enhanced by interaction
with human operators.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Shorten, Connor</author><author>Khoshgoftaar, Taghi M</author></authors></contributors><titles><title>A survey on Image Data Augmentation for Deep Learning</title><secondary-title>JOURNAL OF BIG DATA</secondary-title></titles><periodical><full-title>JOURNAL OF BIG DATA</full-title></periodical><volume>6</volume><issue>1</issue><keywords><keyword>Data Augmentation; Big data; Image data; Deep Learning; GANs</keyword></keywords><dates><year>2019</year></dates><electronic-resource-num>10.1186/s40537-019-0197-0</electronic-resource-num><urls/><abstract>Deep convolutional neural networks have performed remarkably well on
many Computer Vision tasks. However, these networks are heavily reliant
on big data to avoid overfitting. Overfitting refers to the phenomenon
when a network learns a function with very high variance such as to
perfectly model the training data. Unfortunately, many application
domains do not have access to big data, such as medical image analysis.
This survey focuses on Data Augmentation, a data-space solution to the
problem of limited data. Data Augmentation encompasses a suite of
techniques that enhance the size and quality of training datasets such
that better Deep Learning models can be built using them. The image
augmentation algorithms discussed in this survey include geometric
transformations, color space augmentations, kernel filters, mixing
images, random erasing, feature space augmentation, adversarial
training, generative adversarial networks, neural style transfer, and
meta-learning. The application of augmentation methods based on GANs are
heavily covered in this survey. In addition to augmentation techniques,
this paper will briefly discuss other characteristics of Data
Augmentation such as test-time augmentation, resolution impact, final
dataset size, and curriculum learning. This survey will present existing
methods for Data Augmentation, promising developments, and meta-level
decisions for implementing Data Augmentation. Readers will understand
how Data Augmentation can improve the performance of their models and
expand limited datasets to take advantage of the capabilities of big
data.</abstract></record><record><database name="top-50-review.enl" path="top-50-review.enl">top-50-review.enl</database><ref-type name="Journal Article">0</ref-type><contributors><authors><author>Belkin, M</author><author>Niyogi, P</author></authors></contributors><titles><title>Laplacian eigenmaps for dimensionality reduction and data representation</title><secondary-title>NEURAL COMPUTATION</secondary-title></titles><periodical><full-title>NEURAL COMPUTATION</full-title></periodical><pages>1373-1396</pages><volume>15</volume><issue>6</issue><keywords/><dates><year>2003</year></dates><electronic-resource-num>10.1162/089976603321780317</electronic-resource-num><urls/><abstract>One of the central problems in machine learning and pattern recognition
is to develop appropriate representations for complex data. We consider
the problem of constructing a representation for data lying on a
low-dimensional manifold embedded in a high-dimensional space. Drawing
on the correspondence between the graph Laplacian, the Laplace Beltrami
operator on the manifold, and the connections to the heat equation, we
propose a geometrically motivated algorithm for representing the
high-dimensional data. The algorithm provides a computationally
efficient approach to nonlinear dimensionality reduction that has
locality-preserving properties and a natural connection to clustering.
Some potential applications and illustrative examples are discussed.</abstract></record></records></xml>
